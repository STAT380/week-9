---
title: "Week 9"
title-block-banner: true
title-block-style: default
format: ipynb
# format: html
---

```{r}
#| echo: false
#| message: false
#| output: false
#| tags: []
#| vscode: {languageId: r}

```
# Agenda
- Auto Diff

- Logistic Regression With Torch

- Classification With NN

#### Packages we will require this week

```{r}
#| echo: false
#| message: false
#| output: false
#| tags: []
#| vscode: {languageId: r}
packages <- c(
    # Old packages
    "ISLR2",
    "dplyr",
    "tidyr",
    "readr",
    "purrr",
    "glmnet",
    "caret",
    "repr",
    # NEW
    "torch",
    "mlbench",
    "tidyverse"
)


# renv::install(packages)
sapply(packages, require, character.only=TRUE)
```

# Recap: Gradient Descent

```{r}
x <- cars$speed
y <- cars$dist

# define the loss function

Loss <- function(b, x, y){
    squares <- (y - b[1] - b[2] * x)^2
    return( mean(squares) )
}

b <- rnorm(2)
Loss(b, cars$speed, cars$dist)

# define a function to compute the gradients

grad <- function(b, Loss, x, y, eps=1e-5){
    b0_up <- Loss( c(b[1] + eps, b[2]), x, y)
    b0_dn <- Loss( c(b[1] - eps, b[2]), x, y)
    
    b1_up <- Loss( c(b[1], b[2] + eps), x, y)
    b1_dn <- Loss( c(b[1], b[2] - eps), x, y)
    
    grad_b0_L <- (b0_up - b0_dn) / (2 * eps)
    grad_b1_L <- (b1_up - b1_dn) / (2 * eps)
    
    return( c(grad_b0_L, grad_b1_L) )
}

grad(b, Loss, cars$speed, cars$dist)

steps <- 9999
L_numeric <- rep(Inf, steps)
eta <- 1e-4
b_numeric <- rep(0.0, 2)

for (i in 1:steps){
    b_numeric <- b_numeric - eta * grad(b_numeric, Loss, cars$speed, cars$dist)
    L_numeric[i] <- Loss(b_numeric, cars$speed, cars$dist)
    if(i %in% c(1:10) || i %% 1000 == 0){
        cat(sprintf("Iteration: %s\t Loss value: %s\n", i, L_numeric[i]))
    }
}
```




# Auto Diff

## Automatic differentiation

The cornerstone of modern machine learning and data-science is to be able to perform **automatic differentiation**, i.e., being able to compute the gradients for **any** function without the need to solve tedious calculus problems. For the more advanced parts of the course (e.g., neural networks), we will be using automatic differentiation libraries to perform gradient descent. 

While there are several libraries for performing these tasks, we will be using the `pyTorch` library for this. The installation procedure can be found [here](https://cran.r-project.org/web/packages/torch/vignettes/installation.html)

The basic steps are:
```R
renv::install("torch")
library(torch)
torch::install_torch()
```

---

### Example 1:

```{r}
#| vscode: {languageId: r}
x <- torch_randn(c(5, 1), requires_grad=TRUE)
x
```
sqrt(sum(as_array(x)^2)^10)

```{r}
#| vscode: {languageId: r}
f <- function(x){
    torch_norm(x)^10
}

y <- f(x)
y
y$backward()
```

$$
\frac{dy}{dx}
$$

```{r}
#| vscode: {languageId: r}
x$grad
```
```{r}
#| vscode: {languageId: r}
(5 * torch_norm(x)^8) * (2 * x)
```

---

### Example 2:

```{r}
#| vscode: {languageId: r}
x <- torch_randn(c(10, 1), requires_grad=T)
y <- torch_randn(c(10, 1), requires_grad=T)

c(x, y)
```

```{r}
#| vscode: {languageId: r}
f <- function(x, y){
    sum(x * y)
}

z <- f(x, y)
z
z$backward()
```

```{r}
#| vscode: {languageId: r}
c(x$grad, y$grad)
```

```{r}
#| vscode: {languageId: r}
c(x - y$grad, y - x$grad)
```

---

### Example 3:

```{r}
#| vscode: {languageId: r}
x <- torch_tensor(cars$speed, dtype = torch_float())
y <- torch_tensor(cars$dist, dtype = torch_float())

plot(x, y)
```

```{r}
#| vscode: {languageId: r}
b <- torch_zeros(c(2,1), dtype=torch_float(), requires_grad = TRUE)
b
```

```{r}
#| vscode: {languageId: r}
loss <- nn_mse_loss()
```

```{r}
#| vscode: {languageId: r}
b <- torch_zeros(c(2,1), dtype=torch_float(), requires_grad = TRUE) # Initializing variables
steps <- 10000 # Specifying the number of optimization steps
L <- rep(Inf, steps) # Keeping track of the loss


eta <- 0.5 # Specifying the learning rate and the optimizer
optimizer <- optim_adam(b, lr=eta)


# Gradient descent optimization over here
for (i in 1:steps){
    y_hat <- x * b[2] + b[1]
    l <- loss(y_hat, y)
    
    L[i] <- l$item()
    optimizer$zero_grad()
    l$backward()
    optimizer$step()
    
    if(i %in% c(1:10) || i %% 200 == 0){
        cat(sprintf("Iteration: %s\t Loss value: %s\n", i, L[i]))
    }
}

```

```{r}
#| vscode: {languageId: r}
options(repr.plot.width=12, repr.plot.height=7)

par(mfrow=c(1, 2))

plot(x, y)
abline(as_array(b), col="red")

plot(L, type="l", col="dodgerblue")
```



```{r}
#| vscode: {languageId: r}
plot(L_numeric[1:100], type="l", col="red")
lines(L[1:100], col="blue")
```

